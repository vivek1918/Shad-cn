{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b341b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, completely clean up existing installations\n",
    "!pip uninstall -y torch torchvision torchaudio transformers trl accelerate peft bitsandbytes\n",
    "\n",
    "# Install specific compatible versions\n",
    "!pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install transformers==4.36.2  # Version known to work with trl 0.7.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a884d133",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate==0.27.2\n",
    "!pip install peft==0.7.1\n",
    "!pip install trl==0.7.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e178185",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bitsandbytes==0.41.3\n",
    "!pip install datasets==2.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa592d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece protobuf huggingface_hub\n",
    "\n",
    "# Restart runtime after installation\n",
    "import os\n",
    "os._exit(00)  # This will restart the runtime in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adedd0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ShadCN Component Generator with LLaMA 3.1 Fine-tuning\n",
    "# Updated version with fixed package versions and error handling\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import platform\n",
    "import json\n",
    "import gc\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --------------------------\n",
    "# Package Installation Setup\n",
    "# --------------------------\n",
    "\n",
    "def check_gpu_info():\n",
    "    \"\"\"Check GPU and CUDA information\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(\"üéÆ GPU Information:\")\n",
    "            lines = result.stdout.split('\\n')\n",
    "            for line in lines:\n",
    "                if 'CUDA Version' in line:\n",
    "                    print(f\"   {line.strip()}\")\n",
    "                elif 'GeForce' in line or 'Tesla' in line or 'Quadro' in line or 'RTX' in line:\n",
    "                    print(f\"   GPU: {line.split('|')[1].strip()}\")\n",
    "            return True\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è No NVIDIA GPU detected or nvidia-smi not available\")\n",
    "        return False\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install package with error handling\"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--no-cache-dir\"])\n",
    "        print(f\"‚úÖ Installed: {package}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to install {package}: {e}\")\n",
    "        return False\n",
    "\n",
    "def uninstall_package(package):\n",
    "    \"\"\"Uninstall package safely\"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", package])\n",
    "        print(f\"üóëÔ∏è Uninstalled: {package}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def fix_torch_installation():\n",
    "    \"\"\"Fix PyTorch installation with proper CUDA support\"\"\"\n",
    "    print(\"üîß Fixing PyTorch installation...\")\n",
    "    \n",
    "    # Check if we're in Colab\n",
    "    try:\n",
    "        import google.colab\n",
    "        IN_COLAB = True\n",
    "        print(\"üìç Detected Google Colab environment\")\n",
    "    except:\n",
    "        IN_COLAB = False\n",
    "        print(\"üìç Local environment detected\")\n",
    "    \n",
    "    # Check GPU availability\n",
    "    has_gpu = check_gpu_info()\n",
    "    \n",
    "    # Clean installation\n",
    "    print(\"üßπ Cleaning existing PyTorch installation...\")\n",
    "    packages_to_remove = [\"torch\", \"torchvision\", \"torchaudio\", \"bitsandbytes\", \"transformers\", \"accelerate\", \"peft\", \"trl\"]\n",
    "    for pkg in packages_to_remove:\n",
    "        uninstall_package(pkg)\n",
    "    \n",
    "    # Install PyTorch based on environment\n",
    "    if IN_COLAB:\n",
    "        # Colab typically has CUDA 11.8 or 12.1\n",
    "        print(\"üì¶ Installing PyTorch for Colab...\")\n",
    "        success = install_package(\"torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118\")\n",
    "        if not success:\n",
    "            # Fallback to CPU version\n",
    "            print(\"‚ö†Ô∏è GPU version failed, installing CPU version...\")\n",
    "            install_package(\"torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\")\n",
    "    else:\n",
    "        # Local installation - try to detect CUDA version\n",
    "        if has_gpu:\n",
    "            print(\"üì¶ Installing PyTorch with CUDA support...\")\n",
    "            success = install_package(\"torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118\")\n",
    "            if not success:\n",
    "                success = install_package(\"torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\")\n",
    "            if not success:\n",
    "                print(\"‚ö†Ô∏è CUDA versions failed, installing CPU version...\")\n",
    "                install_package(\"torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\")\n",
    "        else:\n",
    "            print(\"üì¶ Installing CPU-only PyTorch...\")\n",
    "            install_package(\"torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\")\n",
    "\n",
    "def install_ml_packages():\n",
    "    \"\"\"Install ML packages with version compatibility\"\"\"\n",
    "    print(\"üì¶ Installing compatible ML packages...\")\n",
    "    \n",
    "    # Core packages with compatible versions\n",
    "    packages = [\n",
    "        \"transformers==4.36.2\",\n",
    "        \"accelerate==0.27.2\",\n",
    "        \"peft==0.7.1\",\n",
    "        \"trl==0.7.10\",\n",
    "        \"datasets==2.15.0\",\n",
    "        \"sentencepiece\",\n",
    "        \"protobuf\",\n",
    "        \"huggingface_hub\",\n",
    "    ]\n",
    "    \n",
    "    for package in packages:\n",
    "        install_package(package)\n",
    "    \n",
    "    # Install bitsandbytes carefully\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"üì¶ Installing bitsandbytes for GPU...\")\n",
    "            install_package(\"bitsandbytes==0.41.3\")\n",
    "        else:\n",
    "            print(\"üì¶ Installing bitsandbytes for CPU...\")\n",
    "            install_package(\"bitsandbytes-cpu\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Installing basic bitsandbytes...\")\n",
    "        install_package(\"bitsandbytes\")\n",
    "\n",
    "# --------------------------\n",
    "# Initial Setup and Checks\n",
    "# --------------------------\n",
    "\n",
    "# Fix installation\n",
    "fix_torch_installation()\n",
    "install_ml_packages()\n",
    "\n",
    "print(\"‚úÖ Installation completed! Testing imports...\")\n",
    "\n",
    "# Test imports with better error handling\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"‚úÖ PyTorch {torch.__version__} imported successfully\")\n",
    "    print(f\"üîç CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"üîç CUDA version: {torch.version.cuda}\")\n",
    "        print(f\"üîç GPU count: {torch.cuda.device_count()}\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"üîç GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå PyTorch import failed: {e}\")\n",
    "    print(\"üí° Try restarting runtime and running again\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Import other libraries after installation\n",
    "try:\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "    print(\"‚úÖ Transformers imported\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Transformers import failed: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "try:\n",
    "    from datasets import Dataset\n",
    "    print(\"‚úÖ Datasets imported\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Datasets import failed: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "try:\n",
    "    from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "    print(\"‚úÖ PEFT imported\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå PEFT import failed: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "try:\n",
    "    from trl import SFTTrainer\n",
    "    print(\"‚úÖ TRL imported\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå TRL import failed: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Try to import bitsandbytes\n",
    "try:\n",
    "    import bitsandbytes as bnb\n",
    "    QUANTIZATION_AVAILABLE = True\n",
    "    print(\"‚úÖ BitsAndBytes available for quantization\")\n",
    "except Exception as e:\n",
    "    QUANTIZATION_AVAILABLE = False\n",
    "    print(f\"‚ö†Ô∏è BitsAndBytes not available: {e}\")\n",
    "\n",
    "# --------------------------\n",
    "# Configuration\n",
    "# --------------------------\n",
    "\n",
    "MODEL_CONFIGS = {\n",
    "    \"llama3.1-8b\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"llama3.1-8b-4bit\": \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"phi3-mini\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    \"qwen2-7b\": \"Qwen/Qwen2-7B-Instruct\",\n",
    "}\n",
    "\n",
    "# Choose model based on available resources\n",
    "if torch.cuda.is_available() and torch.cuda.get_device_properties(0).total_memory > 12e9:  # >12GB VRAM\n",
    "    MODEL_NAME = MODEL_CONFIGS[\"llama3.1-8b-4bit\"]\n",
    "    MAX_SEQ_LENGTH = 2048\n",
    "    BATCH_SIZE = 1\n",
    "    print(\"üéØ Using LLaMA 3.1 8B (4-bit) - High-end setup\")\n",
    "elif torch.cuda.is_available():\n",
    "    MODEL_NAME = MODEL_CONFIGS[\"phi3-mini\"]\n",
    "    MAX_SEQ_LENGTH = 1024\n",
    "    BATCH_SIZE = 1\n",
    "    print(\"üéØ Using Phi-3 Mini - Medium setup\")\n",
    "else:\n",
    "    MODEL_NAME = MODEL_CONFIGS[\"phi3-mini\"]\n",
    "    MAX_SEQ_LENGTH = 512\n",
    "    BATCH_SIZE = 1\n",
    "    print(\"üéØ Using Phi-3 Mini CPU mode - Basic setup\")\n",
    "\n",
    "# Training configuration\n",
    "OUTPUT_DIR = \"./shadcn-component-generator\"\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "MAX_STEPS = 100  # Reduced for testing\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "print(f\"üìä Configuration:\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Max sequence length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Max steps: {MAX_STEPS}\")\n",
    "\n",
    "# --------------------------\n",
    "# Dataset and Training Setup\n",
    "# --------------------------\n",
    "\n",
    "def create_sample_dataset():\n",
    "    \"\"\"Create a comprehensive sample dataset for ShadCN components\"\"\"\n",
    "    components_data = [\n",
    "        {\n",
    "            \"component_name\": \"button\",\n",
    "            \"description\": \"Create a versatile button component with multiple variants\",\n",
    "            \"registry_json\": {\n",
    "                \"name\": \"button\",\n",
    "                \"type\": \"component\",\n",
    "                \"dependencies\": [\"class-variance-authority\", \"clsx\"],\n",
    "                \"props\": {\n",
    "                    \"variant\": {\n",
    "                        \"type\": \"enum\",\n",
    "                        \"values\": [\"default\", \"destructive\", \"outline\", \"secondary\", \"ghost\", \"link\"],\n",
    "                        \"default\": \"default\"\n",
    "                    },\n",
    "                    \"size\": {\n",
    "                        \"type\": \"enum\", \n",
    "                        \"values\": [\"default\", \"sm\", \"lg\", \"icon\"],\n",
    "                        \"default\": \"default\"\n",
    "                    }\n",
    "                },\n",
    "                \"css\": {\n",
    "                    \"base\": \"inline-flex items-center justify-center rounded-md text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50\",\n",
    "                    \"variants\": {\n",
    "                        \"default\": \"bg-primary text-primary-foreground hover:bg-primary/90\",\n",
    "                        \"destructive\": \"bg-destructive text-destructive-foreground hover:bg-destructive/90\",\n",
    "                        \"outline\": \"border border-input bg-background hover:bg-accent hover:text-accent-foreground\",\n",
    "                        \"secondary\": \"bg-secondary text-secondary-foreground hover:bg-secondary/80\",\n",
    "                        \"ghost\": \"hover:bg-accent hover:text-accent-foreground\",\n",
    "                        \"link\": \"text-primary underline-offset-4 hover:underline\"\n",
    "                    },\n",
    "                    \"sizes\": {\n",
    "                        \"default\": \"h-10 px-4 py-2\",\n",
    "                        \"sm\": \"h-9 rounded-md px-3\",\n",
    "                        \"lg\": \"h-11 rounded-md px-8\", \n",
    "                        \"icon\": \"h-10 w-10\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        # ... (rest of your dataset components remain the same)\n",
    "    ]\n",
    "    \n",
    "    return components_data\n",
    "\n",
    "def format_training_prompt(example):\n",
    "    \"\"\"Format training examples for the model\"\"\"\n",
    "    component_name = example[\"component_name\"]\n",
    "    description = example[\"description\"]\n",
    "    registry_json = example[\"registry_json\"]\n",
    "    \n",
    "    prompt = f\"\"\"Create a ShadCN/UI component registry for: {component_name}\n",
    "\n",
    "Description: {description}\n",
    "\n",
    "Component Registry JSON:\n",
    "{json.dumps(registry_json, indent=2)}\"\"\"\n",
    "    \n",
    "    return {\"text\": prompt}\n",
    "\n",
    "def load_model_and_tokenizer():\n",
    "    \"\"\"Load model and tokenizer with error handling\"\"\"\n",
    "    try:\n",
    "        print(f\"üîÑ Loading model: {MODEL_NAME}\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "        \n",
    "        # Set padding token\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.padding_side = \"right\"\n",
    "        \n",
    "        print(\"‚úÖ Tokenizer loaded\")\n",
    "        \n",
    "        # Configure quantization if available\n",
    "        model_kwargs = {\n",
    "            \"trust_remote_code\": True,\n",
    "            \"torch_dtype\": torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        }\n",
    "        \n",
    "        if QUANTIZATION_AVAILABLE and torch.cuda.is_available():\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True\n",
    "            )\n",
    "            model_kwargs[\"quantization_config\"] = bnb_config\n",
    "            model_kwargs[\"device_map\"] = \"auto\"\n",
    "            print(\"‚úÖ Using 4-bit quantization\")\n",
    "        \n",
    "        # Load model\n",
    "        model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, **model_kwargs)\n",
    "        print(\"‚úÖ Model loaded\")\n",
    "        \n",
    "        # Apply LoRA if using quantization\n",
    "        if QUANTIZATION_AVAILABLE and torch.cuda.is_available():\n",
    "            model = prepare_model_for_kbit_training(model)\n",
    "            \n",
    "            peft_config = LoraConfig(\n",
    "                r=16,\n",
    "                lora_alpha=32,\n",
    "                target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "                lora_dropout=0.05,\n",
    "                bias=\"none\",\n",
    "                task_type=\"CAUSAL_LM\"\n",
    "            )\n",
    "            \n",
    "            model = get_peft_model(model, peft_config)\n",
    "            print(\"‚úÖ LoRA configuration applied\")\n",
    "        \n",
    "        return model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load model: {e}\")\n",
    "        raise\n",
    "\n",
    "def train_model():\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    try:\n",
    "        # Load model and tokenizer\n",
    "        model, tokenizer = load_model_and_tokenizer()\n",
    "        \n",
    "        # Create dataset\n",
    "        print(\"üìä Creating training dataset...\")\n",
    "        sample_data = create_sample_dataset()\n",
    "        formatted_data = [format_training_prompt(example) for example in sample_data]\n",
    "        \n",
    "        # Convert to Dataset object\n",
    "        dataset = Dataset.from_list(formatted_data)\n",
    "        print(f\"‚úÖ Dataset created with {len(dataset)} examples\")\n",
    "        \n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            max_steps=MAX_STEPS,\n",
    "            logging_steps=10,\n",
    "            save_steps=50,\n",
    "            warmup_steps=10,\n",
    "            optim=\"adamw_hf\",\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            dataloader_drop_last=True,\n",
    "            remove_unused_columns=False,\n",
    "            report_to=\"none\",\n",
    "            seed=42,\n",
    "        )\n",
    "        \n",
    "        # Create trainer\n",
    "        trainer = SFTTrainer(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            train_dataset=dataset,\n",
    "            dataset_text_field=\"text\",\n",
    "            max_seq_length=MAX_SEQ_LENGTH,\n",
    "            args=training_args,\n",
    "            packing=False,\n",
    "        )\n",
    "        \n",
    "        print(\"üöÄ Starting training...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save model\n",
    "        trainer.save_model(OUTPUT_DIR)\n",
    "        tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "        \n",
    "        print(f\"‚úÖ Training completed! Model saved to {OUTPUT_DIR}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Training failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def test_generation():\n",
    "    \"\"\"Test component generation\"\"\"\n",
    "    try:\n",
    "        print(\"üß™ Testing component generation...\")\n",
    "        \n",
    "        # Load trained model if available\n",
    "        model_path = OUTPUT_DIR if Path(OUTPUT_DIR).exists() else MODEL_NAME\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "        )\n",
    "        \n",
    "        # Test prompts\n",
    "        test_prompts = [\n",
    "            \"Create a badge component with different variants and sizes\",\n",
    "            \"Create a tooltip component with smooth animations\",\n",
    "            \"Create a progress bar component with customizable styling\"\n",
    "        ]\n",
    "        \n",
    "        for prompt in test_prompts:\n",
    "            print(f\"\\nüìù Prompt: {prompt}\")\n",
    "            \n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=300,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            response = generated_text[len(prompt):].strip()\n",
    "            \n",
    "            print(\"Generated:\")\n",
    "            print(response[:200] + \"...\" if len(response) > 200 else response)\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Generation test failed: {e}\")\n",
    "\n",
    "# --------------------------\n",
    "# Main Execution\n",
    "# --------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üé® SHADCN COMPONENT GENERATOR WITH LLAMA 3.1\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Run training\n",
    "    success = train_model()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\nüéâ Training completed successfully!\")\n",
    "        test_generation()\n",
    "    else:\n",
    "        print(\"\\n‚ùå Training failed. Check the error messages above.\")\n",
    "    \n",
    "    print(f\"\\n‚ú® ShadCN Generator ready!\")\n",
    "    print(f\"üìÅ Model saved in: {OUTPUT_DIR}\")\n",
    "    print(\"üöÄ Ready to generate ShadCN components!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
